@inproceedings{roberts15,
  author    = {Kirk Roberts and
               Matthew S. Simpson and
               Ellen M. Voorhees and
               William R. Hersh},
  title     = {Overview of the {TREC} Clinical Decision Support Track},
  booktitle = {TREC 2015},
  year      = {2015},
  crossref  = {DBLP:conf/trec/2015},
  timestamp = {Fri, 15 Apr 2016 15:28:17 +0200},
  biburl    = {http://dblp.dagstuhl.de/rec/bib/conf/trec/RobertsSVH15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
  url       = {http://trec.nist.gov/pubs/trec24/papers/Overview-CL.pdf},
  booktitle = {Proceedings of The Twenty-Fourth Text REtrieval Conference, {TREC}
               2015, Gaithersburg, Maryland, USA, November 17-20, 2015},

@phdthesis{santos13thesis,
  author = {Rodrygo L. T. Santos},
  title = {Explicit web search result diversification},
  school = {School of Computing Science, University of Glasgow},
  year = {2013},
  address = {Glasgow, UK},
}

@inproceedings{glater17,
  author = {Rafael Glater and Rodrygo L. T. Santos and Nivio Ziviani},
  title = {Intent-aware semantic query annotation},
  booktitle = {Proceedings of the 40th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
  year = {2017},
  isbn = {},
  address = {Tokyo, Japan},
  pages = {},
  doi = {10.1145/3077136.3080825},
  publisher = {ACM},
}

@inproceedings{zuccon16,
  title={Understandability Biased Evaluation for Information Retrieval},
  author={Guido Zuccon},
  booktitle={ECIR},
  year={2016}
}

@inproceedings{zuccon17tutorial,
 author = {Zuccon, Guido and Koopman, Bevan},
 title = {SIGIR 2017 Tutorial on Health Search (HS2017): A Full-day from Consumers to Clinicians},
 series = {SIGIR '17},
 year = {2017},
 booktitle = {SIGIR 2017},
 isbn = {978-1-4503-5022-8},
 location = {Shinjuku, Tokyo, Japan},
 pages = {1415--1418},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/3077136.3082061},
 doi = {10.1145/3077136.3082061},
 acmid = {3082061},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {consumer health search, domain specific information retrieval, domain specific search, health information retrieval, health information seeking, health search, medical information retrieval},
}
 booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
 INPROCEEDINGS{zuccon17tutorial, 
author={G. Zuccon and B. Koopman}, 
booktitle={Proceedings of international ACM SIGIR conference on Research and development in Information Retrieval}, 
title={SIGIR 2017 tutorial on health search (HS2017): A full-day from consumers to clinicians}, 
year={2017}, 
month={August},
address   = {Tokyo, Japan},
}

@article{alsulmi16bibm1,
abstract = {—Clinical Decision Support Systems (CDS) are built to make biomedical literature efficiently accessible to clinicians. The typical queries submitted to these systems are medical cases which describe various aspects of a patient health in-cluding symptoms, lab tests, and previous treatments. Because these queries are expected to be long and may contain some ambiguous words, performing some reformulation tasks to those queries (e.g., removing and re-weighting some of the query terms) could have a substantial beneficial effect on the performance of CDS systems. In this paper we describe several such reformulations and use them to achieve a major improvement in retrieval effectiveness.},
author = {Alsulmi, Mohammad and Carterette, Ben},
isbn = {9781509016105},
journal = {IEEE International Conference on Bioinformatics and Biomedicine (BIBM)},
keywords = {-information retrieval,clinical decision support,query reformulations},
pages = {694--698},
title = {{Improving Clinical Case Search Using Semantic Based Query Reformulations}},
year = {2016}
}

@INPROCEEDINGS{alsulmi16bibm2, 
author={M. Alsulmi and B. Carterette}, 
booktitle={2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)}, 
title={Learning to predict the performance of clinical queries using an integrated approach}, 
year={2016}, 
pages={930-937}, 
keywords={decision support systems;learning (artificial intelligence);medical computing;query formulation;TREC clinical decision support track;clinical queries performance;cross-validated training;integrated approach;prediction learning;query predictors;retrieval effectiveness;system features;user queries;Logistics;Surgery;clinical decision support systems;information retrieval;query performance prediction}, 
doi={10.1109/BIBM.2016.7822649}, 
month={Dec},
}

@article{aronson01,
abstract = {The UMLS Metathesaurus, the largest thesaurus in the biomedical domain, provides a representation of biomedical knowledge consisting of concepts classified by semantic type and both hierarchical and non-hierarchical relationships among the concepts. This knowledge has proved useful for many applications including decision support systems, management of patient records, information retrieval (IR) and data mining. Gaining effective access to the knowledge is critical to the success of these applications. This paper describes MetaMap, a program developed at the National Library of Medicine (NLM) to map biomedical text to the Metathesaurus or, equivalently, to discover Metathesaurus concepts referred to in text. MetaMap uses a knowledge intensive approach based on symbolic, natural language processing (NLP) and computational linguistic techniques. Besides being applied for both IR and data mining applications, MetaMap is one of the foundations of NLM's Indexing Initiative System which is being applied to both semi-automatic and fully automatic indexing of the biomedical literature at the library.},
author = {Aronson, a R},
doi = {D010001275 [pii]},
isbn = {1531-605X (Print)$\backslash$n1531-605X (Linking)},
issn = {1531-605X},
journal = {AMIA Annual Symposium Proceedings},
pages = {17--21},
pmid = {11825149},
title = {{Effective mapping of biomedical text to the UMLS Metathesaurus: the MetaMap program.}},
year = {2001}
}

@InProceedings{amini12,
author = {Iman Amini and David Martinez and Diego Molla},
title = {Overview of the ALTA 2012 Shared Task},
booktitle = {Proceedings of the Australasian Language Technology Association Workshop 2012},
month     = {December},
year      = {2012},
address   = {Dunedin, New Zealand},
pages = {124--129},
url = http://www.aclweb.org/anthology/U/U12/U12-2017
}

@inproceedings{kordan16,
abstract = {Abstract Accurately answering verbose queries that describe a clinical case and aim at finding articles in a collection of medical literature requires capturing many explicit and latent aspects of complex information needs underlying such queries. Proper representation of these aspects often requires query analysis to identify the most important query concepts as well as query transformation by adding new concepts to a query, which can be extracted ...
},
author = {Balaneshin-kordan, Saeid and Kotov, Alexander},
booktitle = {Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval},
doi = {10.1145/2970398.2970418},
isbn = {9781450344975},
pages = {241--250},
title = {{Optimization Method for Weighting Explicit and Latent Concepts in Clinical Decision Support Queries}},
url = {http://dl.acm.org/citation.cfm?doid=2970398.2970418{\%}0Apapers3://publication/doi/10.1145/2970398.2970418},
year = {2016}
}

@article{boudin10naacl,
abstract = {In evidence-based medicine, clinical questions involve four aspects: Patient/Problem (P), Intervention (I), Comparison (C) and Outcome (O), known as PICO elements. In this paper we present a method that extends the language modeling approach to incorporate both document structure and PICO query formulation. We present an analysis of the distribution of PICO elements in medical abstracts that motivates the use of a location-based weighting strategy. In experiments carried out on a collection of 1.5 million abstracts, the method was found to lead to an improvement of roughly 60{\%} in MAP and 70{\%} in P@10 as compared to state-of-the-art methods.},
author = {Boudin, Florian and Nie, Jian-yun and Dawes, Martin},
isbn = {1932432655},
journal = {Annual Conference of the North American Chapter of the Association for Computational Linguistics},
number = {June},
pages = {822--830},
title = {{Clinical Information Retrieval using Document and PICO Structure}},
year = {2010}
}

@inproceedings{boudin10emnlp,
abstract = {The PECO framework is a knowledge representation for formulating clinical questions. Queries are decomposed into four aspects, which are Patient-Problem (P), Exposure (E), Comparison (C) and Outcome (O). However, no test collection is available to evaluate such framework in information retrieval. In this work, we first present the construction of a large test collection extracted from systematic literature reviews. We then describe an analysis of the distribution of PECO elements throughout the relevant documents and propose a language modeling approach that uses these distributions as a weighting strategy. In our experiments carried out on a collection of 1.5 million documents and 423 queries, our method was found to lead to an improvement of 28{\%} in MAP and 50{\%} in P@5, as compared to the state-of-the-art method. {\textcopyright} 2010 Association for Computational Linguistics.},
author = {Boudin, Florian and Nie, Jian Yun and Dawes, Martin},
booktitle = {EMNLP 2010 - Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference},
isbn = {1932432868},
pages = {108--115},
title = {{Positional language models for clinical information retrieval}},
year = {2010}
}

@inproceedings{cartright11,
abstract = {We study information goals and patterns of attention in explorato- ry search for health information on the Web, reporting results of a large-scale log-based study. We examine search activity associat- ed with the goal of diagnosing illness from symptoms versus more general information-seeking about health and illness. We decom- pose exploratory health search into evidence-based and hypothe- sis-directed information seeking. Evidence-based search centers on the pursuit of details and relevance of signs and symptoms. Hypothesis-directed search includes the pursuit of content on one or more illnesses, including risk factors, treatments, and therapies for illnesses, and on the discrimination among different diseases under the uncertainty that exists in advance of a confirmed diag- nosis. These different goals of exploratory health search are not independent, and transitions can occur between them within or across search sessions. We construct a classifier that identifies medically-related search sessions in log data. Given a set of search sessions flagged as health-related, we show how we can identify different intentions persisting as foci of attention within those sessions. Finally, we discuss how insights about foci dy- namics can help us better understand exploratory health search behavior},
author = {Cartright, Marc-Allen and White, Ryen W. and Horvitz, Eric},
booktitle = {Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval - SIGIR '11},
doi = {10.1145/2009916.2009929},
isbn = {9781450307574},
pages = {65},
title = {{Intentions and attention in exploratory health search}},
url = {http://portal.acm.org/citation.cfm?doid=2009916.2009929},
year = {2011}
}

@article{chapman01,
abstract = {Narrative reports in medical records contain a wealth of information that may augment structured data for managing patient information and predicting trends in diseases. Pertinent negatives are evident in text but are not usually indexed in structured databases. The objective of the study reported here was to test a simple algorithm for determining whether a finding or disease mentioned within narrative medical reports is present or absent. We developed a simple regular expression algorithm called NegEx that implements several phrases indicating negation, filters out sentences containing phrases that falsely appear to be negation phrases, and limits the scope of the negation phrases. We compared NegEx against a baseline algorithm that has a limited set of negation phrases and a simpler notion of scope. In a test of 1235 findings and diseases in 1000 sentences taken from discharge summaries indexed by physicians, NegEx had a specificity of 94.5{\%} (versus 85.3{\%} for the baseline), a positive predictive value of 84.5{\%} (versus 68.4{\%} for the baseline) while maintaining a reasonable sensitivity of 77.8{\%} (versus 88.3{\%} for the baseline). We conclude that with little implementation effort a simple regular expression algorithm for determining whether a finding or disease is absent can identify a large portion of the pertinent negatives from discharge summaries.},
author = {Chapman, Wendy W. and Bridewell, Will and Hanbury, Paul and Cooper, Gregory F. and Buchanan, Bruce G.},
doi = {10.1006/jbin.2001.1029},
isbn = {1532-0464 (Print)$\backslash$r1532-0464},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
number = {5},
pages = {301--310},
pmid = {12123149},
title = {{A Simple Algorithm for Identifying Negated Findings and Diseases in Discharge Summaries}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1532046401910299},
volume = {34},
year = {2001}
}

@article{fushman07,
abstract = {The combination of recent developments in question-answering research and the availability of unparalleled resources developed specifically for automatic semantic processing of text in the medical domain provides a unique opportunity to explore complex question answering in the domain of clinical medicine. This article presents a system designed to satisfy the information needs of physicians practicing evidence-based medicine. We have developed a series of knowledge extractors, which employ a combination of knowledge-based and statistical techniques, for automatically identifying clinically relevant aspects of MEDLINE abstracts. These extracted elements serve as the input to an algorithm that scores the relevance of citations with respect to structured representations of information needs, in accordance with the principles of evidence-based medicine. Starting with an initial list of citations retrieved by PubMed, our system can bring relevant abstracts into higher ranking positions, and from these abstracts generate responses that directly answer physicians' questions. We describe three separate evaluations: one focused on the accuracy of the knowledge extractors, one conceptualized as a document reranking task, and finally, an evaluation of answers by two physicians. Experiments on a collection of real-world clinical questions show that our approach significantly outperforms the already competitive PubMed baseline.},
author = {Demner-Fushman, Dina and Lin, Jimmy},
doi = {10.1162/coli.2007.33.1.63},
isbn = {0891-2017},
issn = {0891-2017},
journal = {Computational Linguistics},
number = {1},
pages = {63--103},
title = {{Answering Clinical Questions with Knowledge-Based and Statistical Techniques}},
url = {http://dl.acm.org/citation.cfm?id=1245134.1245139},
volume = {33},
year = {2007}
}

@article{harkema09,
abstract = {In this paper we describe an algorithm called ConText for determining whether clinical conditions mentioned in clinical reports are negated, hypothetical, historical, or experienced by someone other than the patient. The algorithm infers the status of a condition with regard to these properties from simple lexical clues occurring in the context of the condition. The discussion and evaluation of the algorithm presented in this paper address the questions of whether a simple surface-based approach which has been shown to work well for negation can be successfully transferred to other contextual properties of clinical conditions, and to what extent this approach is portable among different clinical report types. In our study we find that ConText obtains reasonable to good performance for negated, historical, and hypothetical conditions across all report types that contain such conditions. Conditions experienced by someone other than the patient are very rarely found in our report set. A comprehensive solution to the problem of determining whether a clinical condition is historical or recent requires knowledge above and beyond the surface clues picked up by ConText. ?? 2009 Elsevier Inc. All rights reserved.},
author = {Harkema, Henk and Dowling, John N. and Thornblade, Tyler and Chapman, Wendy W.},
doi = {10.1016/j.jbi.2009.05.002},
isbn = {1532-0480 (Electronic) 1532-0480 (Linking)},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
keywords = {Clinical reporting,Natural language processing,Negation,Temporality},
number = {5},
pages = {839--851},
pmid = {19435614},
title = {{ConText: An algorithm for determining negation, experiencer, and temporal status from clinical reports}},
volume = {42},
year = {2009}
}

@Inbook{he16,
author="He, Yun
and Hu, Qinmin
and Song, Yang
and He, Liang",
title="Estimating Probability Density of Content Types for Promoting Medical Records Search",
bookTitle="Advances in Information Retrieval: 38th European Conference on IR Research, ECIR 2016, Padua, Italy, March 20--23, 2016. Proceedings",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="252--263",
isbn="978-3-319-30671-1",
doi="10.1007/978-3-319-30671-1_19",
url="http://dx.doi.org/10.1007/978-3-319-30671-1_19"
}

@book{hersh09,
  abstract = {There continue to be enormous changes and advancements in the indexing and retrieval of online health information, fueled by the growth of the Internet and World Wide Web. This has enabled the field of information retrieval, also known as search, to have substantial impact on diverse types of information in areas from consumer health to clinical medicine to genomics. Totally rewritten, this new edition provides an overview of the theory, practical applications, evaluation, and research directions of all aspects of health and biomedical information retrieval systems. The book is organized into three sections covering basic principles, state of the art systems, and research directions. The book demystifies the jargon and defines where current applications and research systems are heading the field in areas such as digital libraries, linkage to electronic health records, and text mining systems. This book details the technical state-of-the-art and research results in health and biomedical information retrieval. The increasing importance of health information retrieval makes the book an invaluable tool for those involved in this area, such as medical informaticians, computer scientists, library/information scientists, IT developers, and leaders of hospitals, companies, and universities.},
  added-at = {2016-03-29T11:26:32.000+0200},
  address = {New York},
  author = {Hersh, William},
  biburl = {https://www.bibsonomy.org/bibtex/288d6f0a1fe10563824d83f3748d9b1cb/flint63},
  doi = {10.1007/978-0-387-78703-9},
  edition = 3,
  file = {SpringerLink:2009/Hersh09.pdf:PDF;Springer Product page:http\://www.springer.com/978-0-387-78702-2:URL;Amazon Search inside:http\://www.amazon.de/gp/reader/038778702X/:URL},
  groups = {public},
  interhash = {5e903a0b222fcaa82365bb6cbbc2b340},
  intrahash = {88d6f0a1fe10563824d83f3748d9b1cb},
  isbn = {978-0-387-78702-2},
  keywords = {01624 103 springer book ai information retrieval language processing health},
  publisher = {Springer},
  timestamp = {2016-05-19T11:35:21.000+0200},
  title = {Information Retrieval: A Health and Biomedical Perspective},
  username = {flint63},
  year = 2009
}

@article{jin06,
abstract = {Background$\backslash$nThe rapid proliferation of biomedical text makes it increasingly difficult for researchers to identify, synthesize, and utilize developed knowledge in their fields of interest. Automated information extraction procedures can assist in the acquisition and management of this knowledge. Previous efforts in biomedical text mining have focused primarily upon named entity recognition of well-defined molecular objects such as genes, but less work has been performed to identify disease-related objects and concepts. Furthermore, promise has been tempered by an inability to efficiently scale approaches in ways that minimize manual efforts and still perform with high accuracy. Here, we have applied a machine-learning approach previously successful for identifying molecular entities to a disease concept to determine if the underlying probabilistic model effectively generalizes to unrelated concepts with minimal manual intervention for model retraining.$\backslash$n$\backslash$nResults$\backslash$nWe developed a named entity recognizer (MTag), an entity tagger for recognizing clinical descriptions of malignancy presented in text. The application uses the machine-learning technique Conditional Random Fields with additional domain-specific features. MTag was tested with 1,010 training and 432 evaluation documents pertaining to cancer genomics. Overall, our experiments resulted in 0.85 precision, 0.83 recall, and 0.84 F-measure on the evaluation set. Compared with a baseline system using string matching of text with a neoplasm term list, MTag performed with a much higher recall rate (92.1{\%} vs. 42.1{\%} recall) and demonstrated the ability to learn new patterns. Application of MTag to all MEDLINE abstracts yielded the identification of 580,002 unique and 9,153,340 overall mentions of malignancy. Significantly, addition of an extensive lexicon of malignancy mentions as a feature set for extraction had minimal impact in performance.$\backslash$n$\backslash$nConclusion$\backslash$nTogether, these results suggest that the identification of disparate biomedical entity classes in free text may be achievable with high accuracy and only moderate additional effort for each new application domain.},
author = {Jin, Yang and McDonald, Ryan T and Lerman, Kevin and Mandel, Mark A and Carroll, Steven and Liberman, Mark Y and Pereira, Fernando C and Winters, Raymond S and White, Peter S},
doi = {10.1186/1471-2105-7-492},
isbn = {1471-2105 (Linking)},
issn = {1471-2105},
journal = {BMC Bioinformatics},
pages = {492},
pmid = {17090325},
title = {{Automated recognition of malignancy mentions in biomedical literature}},
url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1657036/{\%}5Cnhttp://www.ncbi.nlm.nih.gov/pmc/articles/PMC1657036/pdf/1471-2105-7-492.pdf},
volume = {7},
year = {2006}
}

@inproceedings{koopman14,
abstract = {We present a study to understand the effect that negated terms (e.g., "no fever") and family history (e.g., "family history of diabetes") have on searching clinical records. Our analysis is aimed at devising the most effective means of handling negation and family history. In doing so, we explicitly represent a clinical record according to its different content types: negated, family history and normal content; the retrieval model weights each of these separately. Empirical evaluation shows that overall the presence of negation harms retrieval effectiveness while family history has little effect. We show negation is best handled by weighting negated content (rather than the common practise of removing or replacing it). However, we also show that many queries benefit from the inclusion of negated content and that negation is optimally handled on a per-query basis. Additional evaluation shows that adaptive handing of negated and family history content can have significant benefits. Copyright 2014 ACM.},
author = {Koopman, B. and Zuccon, G.},
booktitle = {SIGIR 2014 - Proceedings of the 37th International ACM SIGIR Conference on Research and Development in Information Retrieval},
doi = {10.1145/2600428.2609487},
isbn = {9781450322591},
keywords = {Experimentation,Measurement},
pages = {971--974},
title = {{Understanding negation and family history to improve clinical information retrieval}},
year = {2014}
}

@inproceedings{koopman16sigir,
 author = {Koopman, Bevan and Zuccon, Guido},
 title = {A Test Collection for Matching Patients to Clinical Trials},
 booktitle = {Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval},
 series = {SIGIR '16},
 year = {2016},
 isbn = {978-1-4503-4069-4},
 location = {Pisa, Italy},
 pages = {669--672},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/2911451.2914672},
 doi = {10.1145/2911451.2914672},
 acmid = {2914672},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {clinical trials, information retrieval, medical information retrieval, test collections},
} 

@Article{koopman16irj,
author="Koopman, Bevan
and Zuccon, Guido
and Bruza, Peter
and Sitbon, Laurianne
and Lawley, Michael",
title="Information retrieval as semantic inference: a Graph Inference model applied to medical search",
journal="Information Retrieval Journal",
year="2016",
volume="19",
number="1",
pages="6--37",
abstract="This paper presents a Graph Inference retrieval model that integrates structured knowledge resources, statistical information retrieval methods and inference in a unified framework. Key components of the model are a graph-based representation of the corpus and retrieval driven by an inference mechanism achieved as a traversal over the graph. The model is proposed to tackle the semantic gap problem---the mismatch between the raw data and the way a human being interprets it. We break down the semantic gap problem into five core issues, each requiring a specific type of inference in order to be overcome. Our model and evaluation is applied to the medical domain because search within this domain is particularly challenging and, as we show, often requires inference. In addition, this domain features both structured knowledge resources as well as unstructured text. Our evaluation shows that inference can be effective, retrieving many new relevant documents that are not retrieved by state-of-the-art information retrieval models. We show that many retrieved documents were not pooled by keyword-based search methods, prompting us to perform additional relevance assessment on these new documents. A third of the newly retrieved documents judged were found to be relevant. Our analysis provides a thorough understanding of when and how to apply inference for retrieval, including a categorisation of queries according to the effect of inference. The inference mechanism promoted recall by retrieving new relevant documents not found by previous keyword-based approaches. In addition, it promoted precision by an effective reranking of documents. When inference is used, performance gains can generally be expected on hard queries. However, inference should not be applied universally: for easy, unambiguous queries and queries with few relevant documents, inference did adversely affect effectiveness. These conclusions reflect the fact that for retrieval as inference to be effective, a careful balancing act is involved. Finally, although the Graph Inference model is developed and applied to medical search, it is a general retrieval model applicable to other areas such as web search, where an emerging research trend is to utilise structured knowledge resources for more effective semantic search.",
issn="1573-7659",
doi="10.1007/s10791-015-9268-9",
url="http://dx.doi.org/10.1007/s10791-015-9268-9"
}

@inproceedings{limsopatham12,
abstract = {In medical records, negative qualifiers, e.g. no or without, are commonly used by health practitioners to identify the absence of a medical condition. Without considering whether the term occurs in a negative or positive context, the sole presence of a query term in a medical record is insufficient to imply that the record is relevant to the query. In this paper, we show how to effectively handle such negation within a medical records information retrieval system. In particular, we propose a term representation that tackles negated language in medical records, which is further extended by considering the dependence of negated query terms. We evaluate our negation handling technique within the search task provided by the TREC Medical Records 2011 track. Our results, which show a significant improvement upon a system that does not consider negated context within records, attest the importance of handling negation.},
author = {Limsopatham, Nut and Macdonald, Craig and McCreadie, Richard and Ounis, Iadh},
booktitle = {Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval},
doi = {10.1145/2348283.2348471},
isbn = {978-1-4503-1472-5},
pages = {1065--1066},
title = {{Exploiting Term Dependence While Handling Negation in Medical Search}},
year = {2012}
}

@Inbook{limsopatham13ecir,
author="Limsopatham, Nut
and Macdonald, Craig
and Ounis, Iadh",
title="A Task-Specific Query and Document Representation for Medical Records Search",
bookTitle="Advances in Information Retrieval: 35th European Conference on IR Research, ECIR 2013, Moscow, Russia, March 24-27, 2013. Proceedings",
year="2013",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="747--751",
isbn="978-3-642-36973-5",
doi="10.1007/978-3-642-36973-5_75",
url="http://dx.doi.org/10.1007/978-3-642-36973-5_75"
}

article{DBLP:journals/corr/LimsopathamMO17,
  author    = {Nut Limsopatham and
               Craig MacDonald and
               Iadh Ounis},
  title     = {Inferring Conceptual Relationships When Ranking Patients},
  journal   = {CoRR},
  volume    = {abs/1702.00171},
  year      = {2017},
  url       = {http://arxiv.org/abs/1702.00171},
  timestamp = {Wed, 07 Jun 2017 14:41:02 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/LimsopathamMO17},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
%TODO this work was resubmitted on ArXiv 17. Why?
@inproceedings{limsopatham13oair,
 author = {Limsopatham, Nut and Macdonald, Craig and Ounis, Iadh},
 title = {Inferring Conceptual Relationships to Improve Medical Records Search},
 booktitle = {Proceedings of the 10th Conference on Open Research Areas in Information Retrieval},
 series = {OAIR '13},
 year = {2013},
 isbn = {978-2-905450-09-8},
 location = {Lisbon, Portugal},
 pages = {1--8},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=2491748.2491750},
 acmid = {2491750},
 publisher = {LE CENTRE DE HAUTES ETUDES INTERNATIONALES D'INFORMATIQUE DOCUMENTAIRE},
 address = {Paris, France, France},
 keywords = {inference, medical records search, query and document representation},
} 

@article{limsopatham13sigir,
abstract = {The complexity of medical terminology raises challenges when searching medical records. For example, 'cancer', 'tumour', and 'neoplasms', which are synonyms, may prevent a traditional search system from retrieving relevant records that contain only synonyms of the query terms. Prior works use bag-of-concepts approaches, to deal with this by representing medical terms sharing the same meanings using concepts from medical resources (e.g. MeSH). The relevance scores are then combined with a traditional bag-of-words representation, when inferring the relevance of medical records. Even though the existing approaches are effective, the predicted retrieval effectiveness of either the bag-of-words or bag-of-concepts representation, which may be used to effectively model the score combination and hence improve retrieval performance, is not taken into account. In this paper, we propose a novel learning framework that models the importance of the bag-of-words and the bag-of-concepts representations, combining their scores on a per-query basis. Our proposed framework leverages retrieval performance predictors, such as the clarity score and AvIDF, calculated on both representations as learning features. We evaluate our proposed framework using the TREC Medical Records track's test collections. As our proposed framework can significantly outperform an existing approach that linearly merges the relevance scores, we conclude that retrieval performance predictors can be effectively leveraged when combining the relevance scores.},
author = {Limsopatham, Nut and Macdonald, Craig and Ounis, Iadh},
doi = {10.1145/2484028.2484177},
isbn = {978-1-4503-2034-4},
journal = {Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval},
keywords = {as,can be referred to,chd,controlled,coronary artery,coronary heart disease,disease,heart disease,medical records search,or,ple,regression,retrieval performance predictors,this means,vocabulary},
pages = {833--836},
title = {{Learning to Combine Representations for Medical Records Search}},
year = {2013}
}

@inproceedings{limsopatham13cikm,
 author = {Limsopatham, Nut and Macdonald, Craig and Ounis, Iadh},
 title = {Learning to selectively rank patients' medical history},
 booktitle = {Proceedings of the 22nd ACM international conference on Conference on information \&\#38; knowledge management},
 series = {CIKM '13},
 year = {2013},
 isbn = {978-1-4503-2263-8},
 location = {San Francisco, California, USA},
 pages = {1833--1836},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/2505515.2507874},
 doi = {10.1145/2505515.2507874},
 acmid = {2507874},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {medical records search, regression-trees, selective ranking approach},
} 

@article{li2016,
abstract = {Before deciding to buy a product, many people tend to consult others' opinions on it. Web provides a perfect platform which$\backslash$n one can get information to find out the advantages and disadvantages of the product of his interest. How to automatically$\backslash$n manage the numerous opinionated documents and then to give suggestions to the potential customers is becoming a research hotspot$\backslash$n recently. Constructing a sentiment resource is one of the vital elements of opinion finding and polarity analysis tasks. For$\backslash$n a specific domain, the sentiment resource can be regarded as a dictionary, which contains a list of product feature words$\backslash$n and several opinion words with sentiment polarity for each feature word. This paper proposes an automatic algorithm to extraction$\backslash$n feature words and opinion words for the sentiment resource. We mine the feature words and opinion words from the comments$\backslash$n on the Web with both NLP technique and statistical method. Left context entropy is proposed to extract unknown feature words;$\backslash$n Adjective rules and background corpus are taken into consideration in the algorithm. Experimental results show the effectiveness$\backslash$n of the proposed automatic sentiment resource construction approach. The proposed method that combines NLP and statistical$\backslash$n techniques is better than using only NLP-based technique. Although the experiment is built on mobile telephone comments in$\backslash$n Chinese, the algorithm is domain independent.},
author = {Zhichao Li and Min Zhang and Shaoping Ma and Bo Zhou and Yu Sun},
doi = {10.1007/978-3-642-04769-5},
isbn = {9783540686330},
issn = {03029743},
journal = {Information Retrieval Technology},
keywords = {distinctness},
number = {July},
pages = {112--123},
title = {{Constraining Word Embeddings by Prior Knowledge – Application to Medical Information Retrieval}},
url = {http://www.springerlink.com/content/8338535381487376},
volume = {5839},
year = {2016}
}

@inproceedings{mishra14,
abstract = {We study time-critical search, where users have urgent information needs in the context of an acute problem. As examples, users may need to know how to stem a severe bleed, help a baby who is choking on a foreign object, or respond to an epileptic seizure. While time-critical situations and actions have been studied in the realm of decision-support systems, little has been done with time-critical search and retrieval, and little direct support is offered by search systems. Critical challenges with time-critical search include accurately inferring when users have urgent needs and providing relevant information that can be understood and acted upon quickly. We leverage surveys and search log data from a large mobile search provider to (a) characterize the use of search engines for time-critical situations, and (b) develop predictive models to accurately predict urgent information needs, given a query and a diverse set of features spanning topical, temporal, behavioral, and geospatial attributes. The methods and findings highlight opportunities for extending search and retrieval to consider the urgency of queries.},
author = {Mishra, Nina and White, Ryen W. and Ieong, Samuel and Horvitz, Eric},
booktitle = {Proceedings of the 37th international ACM SIGIR conference on Research {\&} development in information retrieval - SIGIR '14},
doi = {10.1145/2600428.2609613},
isbn = {9781450322577},
pages = {747--756},
title = {{Time-critical search}},
url = {http://dl.acm.org/citation.cfm?doid=2600428.2609613},
year = {2014}
}

@article{palotti16,
abstract = {The internet is an important source of med- ical knowledge for everyone, from laypeople to medical professionals. We investigate how these two extremes, in terms of user groups, have distinct needs and ex- hibit significantly different search behaviour. We make use of query logs in order to study various aspects of these two kinds of users. The logs from America On- line (AOL), Health on the Net (HON), Turning Re- search Into Practice (TRIP) and American Roentgen Ray Society (ARRS) GoldMiner were divided into three sets: (1) laypeople, (2) medical professionals (such as physicians or nurses) searching for health content and (3) users not seeking health advice. Several analyses are made focusing on discovering how users search and what they are most interested in. One possible outcome of our analysis is a classifier to infer user expertise, which was built. We show the results and analyse the feature set used to infer expertise. We conclude that medical experts are more persistent, interacting more with the search engine. Also, our study reveals that, conversely to what is stated in much of the literature, the main focus of users, both laypeople and profession- als, is on disease rather than symptoms. The results of this article, especially through the classifier built, could be used to detect specific user groups and then adapt search results to the user group.},
author = {Palotti, Jo{\~{a}}o and Hanbury, Allan and M{\"{u}}ller, Henning and Kahn, Charles E.},
doi = {10.1007/s10791-015-9269-8},
issn = {15737659},
journal = {Information Retrieval},
keywords = {Health search,Query log analysis,User behavior},
number = {1-2},
pages = {189--224},
title = {{How users search and what they search for in the medical domain: Understanding laypeople and experts through query logs}},
volume = {19},
year = {2016}
}

@Article{roberts16,
author="Roberts, Kirk
and Simpson, Matthew
and Demner-Fushman, Dina
and Voorhees, Ellen
and Hersh, William",
title="State-of-the-art in biomedical literature retrieval for clinical cases: a survey of the TREC 2014 CDS track",
journal="Information Retrieval Journal",
year="2016",
volume="19",
number="1",
pages="113--148",
abstract="Providing access to relevant biomedical literature in a clinical setting has the potential to bridge a critical gap in evidence-based medicine. Here, our goal is specifically to provide relevant articles to clinicians to improve their decision-making in diagnosing, treating, and testing patients. To this end, the TREC 2014 Clinical Decision Support Track evaluated a system's ability to retrieve relevant articles in one of three categories (Diagnosis, Treatment, Test) using an idealized form of a patient medical record. Over 100 submissions from over 25 participants were evaluated on 30 topics, resulting in over 37k relevance judgments. In this article, we provide an overview of the task, a survey of the information retrieval methods employed by the participants, an analysis of the results, and a discussion on the future directions for this challenging yet important task.",
issn="1573-7659",
doi="10.1007/s10791-015-9259-x",
url="http://dx.doi.org/10.1007/s10791-015-9259-x"
}

@Inbook{soldaini15,
author="Soldaini, Luca
and Cohan, Arman
and Yates, Andrew
and Goharian, Nazli
and Frieder, Ophir",
editor="Hanbury, Allan
and Kazai, Gabriella
and Rauber, Andreas
and Fuhr, Norbert",
title="Retrieving Medical Literature for Clinical Decision Support",
bookTitle="Advances in Information Retrieval: 37th European Conference on IR Research, ECIR 2015, Vienna, Austria, March 29 - April 2, 2015. Proceedings",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="538--549",
isbn="978-3-319-16354-3",
doi="10.1007/978-3-319-16354-3_59",
url="http://dx.doi.org/10.1007/978-3-319-16354-3_59"
}

@article{soldaini16medir,
abstract = {Entity extraction is a fundamental step in many health in-formatics systems. In recent years, tools such as MetaMap and cTAKES have been widely used for medical concept ex-traction on medical literature and clinical notes; however, relatively little interest has been placed on their scalabil-ity to large datasets. In this work, we present QuickUMLS: a fast, unsupervised, approximate dictionary matching algo-rithm for medical concept extraction. The proposed method achieves similar precision and recall of state-of-the-art sys-tems on two clinical notes corpora, and outperforms MetaMap and cTAKES on a dataset of consumer drug reviews. More importantly, it is up to 135 times faster than both systems.},
author = {Soldaini, Luca and Goharian, Nazli},
journal = {Medical Information Retrieval (MedIR) Workshop on SIGIR 2016},
keywords = {biomedical mining,concept extraction,health informatics},
title = {{QuickUMLS: a fast, unsupervised approach for medical concept extraction}},
year = {2016}
}

@Inbook{soldaini17,
author="Soldaini, Luca
and Goharian, Nazli",
title="Learning to Rank for Consumer Health Search: A Semantic Approach",
bookTitle="Advances in Information Retrieval: 39th European Conference on IR Research, ECIR 2017, Aberdeen, UK, April 8-13, 2017, Proceedings",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="640--646",
isbn="978-3-319-56608-5",
doi="10.1007/978-3-319-56608-5_60",
url="http://dx.doi.org/10.1007/978-3-319-56608-5_60"
}

@article{soldaini16ir,
abstract = {The majority of Internet users search for medical information online; however, many do not have an adequate medical vocabulary. Users might have difficulties finding the most authoritative and useful information because they are unfamiliar with the appropriate medical expressions describing their condition; consequently, they are unable to adequately satisfy their information need. We investigate the utility of bridging the gap between layperson and expert vocabularies; our approach adds the most appropriate expert expression to queries submitted by users, a task we call query clarification. We evaluated the impact of query clarification. Using three different synonym mappings and conducting two task-based retrieval studies, users were asked to answer medically-related questions using interleaved results from a major search engine. Our results show that the proposed system was preferred by users and helped them answer medical concerns correctly more often, with up to a 7 {\%} increase in correct answers over an unmodified query. Finally, we introduce a supervised classifier to select the most appropriate synonym mapping for each query, which further increased the fraction of correct answers (12 {\%}).},
author = {Soldaini, Luca and Yates, Andrew and Yom-Tov, Elad and Frieder, Ophir and Goharian, Nazli},
doi = {10.1007/s10791-015-9258-y},
isbn = {1386-4564},
issn = {15737659},
journal = {Information Retrieval},
keywords = {Health search,Medical informatics,Personalized search,Query clarification},
number = {1-2},
pages = {149--173},
title = {{Enhancing web search in the medical domain via query clarification}},
volume = {19},
year = {2016}
}

@article{uzuner10,
abstract = {The Third i2b2 Workshop on Natural Language Processing Challenges for Clinical Records focused on the identification of medications, their dosages, modes (routes) of administration, frequencies, durations, and reasons for administration in discharge summaries. This challenge is referred to as the medication challenge. For the medication challenge, i2b2 released detailed annotation guidelines along with a set of annotated discharge summaries. Twenty teams representing 23 organizations and nine countries participated in the medication challenge. The teams produced rule-based, machine learning, and hybrid systems targeted to the task. Although rule-based systems dominated the top 10, the best performing system was a hybrid. Of all medication-related fields, durations and reasons were the most difficult for all systems to detect. While medications themselves were identified with better than 0.75 F-measure by all of the top 10 systems, the best F-measure for durations and reasons were 0.525 and 0.459, respectively. State-of-the-art natural language processing systems go a long way toward extracting medication names, dosages, modes, and frequencies. However, they are limited in recognizing duration and reason fields and would benefit from future research.},
author = {Uzuner, {\"{O}}zlem and Solti, Imre and Cadag, Eithon},
doi = {10.1136/jamia.2010.003947},
issn = {1067-5027},
journal = {Journal of the American Medical Informatics Association},
number = {5},
pages = {514--518},
pmid = {20819854},
title = {{Extracting medication information from clinical text}},
url = {https://academic.oup.com/jamia/article-lookup/doi/10.1136/jamia.2010.003947},
volume = {17},
year = {2010}
}

@article{wallace16,
abstract = {Systematic reviews underpin Evidence Based Medicine (EBM) by addressing precise clinical questions via comprehensive synthesis of all relevant published evidence. Authors of systematic reviews typically define a Population/Problem, Intervention, Comparator, and Outcome (a PICO criteria) of interest, and then retrieve, appraise and synthesize results from all reports of clinical trials that meet these criteria. Identifying PICO elements in the full-texts of trial reports is thus a critical yet time-consuming step in the systematic review process. We seek to expedite evidence synthesis by developing machine learning models to automatically extract sentences from articles relevant to PICO elements. Collecting a large corpus of training data for this task would be prohibitively expensive. Therefore, we derive distant supervision (DS) with which to train models using previously conducted reviews. DS entails heuristically deriving 'soft' labels from an available structured resource. However, we have access only to unstructured, free-text summaries of PICO elements for corresponding articles; we must derive from these the desired sentence-level annotations.},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Wallace, Byron C and Kuiper, Jo{\"{e}}l and Sharma, Aakash and Zhu, Mingxi and Marshall, Iain J and Kuiper, Joel},
doi = {10.1161/CIRCRESAHA.116.303790.The},
eprint = {15334406},
isbn = {0324141122},
issn = {15337928},
journal = {Journal of Machine Learning Research},
pages = {1--25},
pmid = {24655651},
title = {{Extracting PICO Sentences from Clinical Trial Reports using Supervised Distant Supervision}},
volume = {17},
year = {2016}
}

@article{white10,
abstract = {Logs of users' searches on Web health topics can exhibit signs of escalation of medical concerns, where initial queries about common symptoms are followed by queries about serious, rare illnesses. We present an effort to predict such escalations based on the structure and content of pages encountered during medical search sessions. We construct and then characterize the performance of classifiers that predict whether an escalation will occur after the access of a page. Our findings have implications for ranking algorithms and the design of search interfaces.},
author = {White, Ryen W. and Horvitz, Eric},
doi = {10.1145/1835449.1835607},
isbn = {9781450301534},
journal = {Proceeding of the 33rd international ACM SIGIR conference on Research and development in information retrieval - SIGIR '10},
pages = {769},
title = {{Predicting escalations of medical queries based on web page structure and content}},
url = {http://dl.acm.org/citation.cfm?id=1835449.1835607},
year = {2010}
}

@article{white09tois,
 author = {White, Ryen W. and Horvitz, Eric},
 title = {Cyberchondria: Studies of the Escalation of Medical Concerns in Web Search},
 journal = {ACM Trans. Inf. Syst.},
 issue_date = {November 2009},
 volume = {27},
 number = {4},
 month = nov,
 year = {2009},
 issn = {1046-8188},
 pages = {23:1--23:37},
 articleno = {23},
 numpages = {37},
 url = {http://doi.acm.org/10.1145/1629096.1629101},
 doi = {10.1145/1629096.1629101},
 acmid = {1629101},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Cyberchondria},
} 

@article{white09amia,
abstract = {The wealth of medical information on the Web makes it convenient for non-experts to conduct their own diagnosis and healthcare assessment based on limited knowledge of signs, symptoms, and disorders. We present the findings of a survey aimed at exploring laypeoples' activities and experiences with using Web search to pursue explanations for symptoms. Survey findings suggest that the Web may influence anxiety levels and behaviors of those searching for information on undiagnosed conditions. A better understanding of consumer experience regarding the use of the Web to interpret symptoms can assist in the refinement of healthcare content and retrieval.},
author = {White, Ryen W and Horvitz, Eric},
issn = {1942-597X},
journal = {AMIA Annual Symposium Proceedings},
keywords = {Adult,Chi-Square Distribution,Data Collection,Diagnostic Self Evaluation,Humans,Hypochondriasis,Hypochondriasis: psychology,Information Storage and Retrieval,Internet,Physician-Patient Relations,Search Engine,Sex Factors,Terminology as Topic},
pages = {696--700},
pmid = {20351943},
title = {{Experiences with web search on medical concerns and self diagnosis.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2815378{\&}tool=pmcentrez{\&}rendertype=abstract},
year = {2009}
}

@inproceedings{zhu12,
abstract = {The increasing prevalence of electronic health records (EHR), along with the needs for enhanced clinical care, presents new challenges to information retrieval (IR). Many clinical decision-making tasks following the philosophy of Evidence-Based Medicine (EBM) rely on the ability to find relevant health records and gather sufficient clinical evidence under severe time constraints. In this work, we present a system built upon statistical IR methods for searching flat-text health records (i.e. the doctors' notes sections of EHR) for patients with particular conditions specified via a keyword query. In particular, we use multiple external repositories for query expansion, and introduce two novel model weighting methods. Cross-validation results show that our system improves a strong baseline by 30{\%} on mean average precision (MAP), and has a promising overall performance when compared with a manual system doing the same task.},
author = {Zhu, D and Carterette, B},
booktitle = {2012 IEEE International Conference on Bioinformatics and Biomedicine},
doi = {10.1109/BIBM.2012.6392667},
isbn = {VO  -},
keywords = {Auditory system,Electronic publishing,Encyclopedias,Internet,Medical diagnostic imaging,biomedical engineering,electronic health records,enhanced clinical care,evidence-based medicine,flat-text health records,health care,health record search improvement,information retrieval,keyword query,medical computing,medical information systems,model weighting methods,multiple external repositories,multiple query expansion collections,precision performance,query expansion,query formulation,query processing,search problems,severe time constraints,statistical IR methods,statistical analysis},
pages = {1--7},
title = {{Improving health records search using multiple query expansion collections}},
year = {2012}
}

@article{zhu14,
title = "Using large clinical corpora for query expansion in text-based cohort identification ",
journal = "Journal of Biomedical Informatics ",
volume = "49",
number = "",
pages = "275 - 281",
year = "2014",
note = "",
issn = "1532-0464",
doi = "https://doi.org/10.1016/j.jbi.2014.03.010",
url = "http://www.sciencedirect.com/science/article/pii/S1532046414000677",
author = "Dongqing Zhu and Stephen Wu and Ben Carterette and Hongfang Liu",
keywords = "Cohort identification",
keywords = "Information retrieval",
keywords = "Query expansion",
keywords = "Clinical text",
keywords = "Electronic medical records ",
abstract = "Abstract In light of the heightened problems of polysemy, synonymy, and hyponymy in clinical text, we hypothesize that patient cohort identification can be improved by using a large, in-domain clinical corpus for query expansion. We evaluate the utility of four auxiliary collections for the Text \{REtrieval\} Conference task of IR-based cohort retrieval, considering the effects of collection size, the inherent difficulty of a query, and the interaction between the collections. Each collection was applied to aid in cohort retrieval from the Pittsburgh \{NLP\} Repository by using a mixture of relevance models. Measured by mean average precision, performance using any auxiliary resource (MAP = 0.386 and above) is shown to improve over the baseline query likelihood model (MAP = 0.373). Considering subsets of the Mayo Clinic collection, we found that after including 2.5 billion term instances, retrieval is not improved by adding more instances. However, adding the Mayo Clinic collection did improve performance significantly over any existing setup, with a system using all four auxiliary collections obtaining the best results (MAP = 0.4223). Because optimal results in the mixture of relevance models would require selective sampling of the collections, the common sense approach of “use all available data” is inappropriate. However, we found that it was still beneficial to add the Mayo corpus to any mixture of relevance models. On the task of IR-based cohort identification, query expansion with the Mayo Clinic corpus resulted in consistent and significant improvements. As such, any \{IR\} query expansion with access to a large clinical corpus could benefit from the additional resource. Additionally, we have shown that more data is not necessarily better, implying that there is value in collection curation. "
}

@Inbook{zuccon15,
author="Zuccon, Guido
and Koopman, Bevan
and Palotti, Jo{\~a}o",
title="Diagnose This If You Can",
bookTitle="Advances in Information Retrieval: 37th European Conference on IR Research, ECIR 2015, Vienna, Austria, March 29 - April 2, 2015. Proceedings",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="562--567",
isbn="978-3-319-16354-3",
doi="10.1007/978-3-319-16354-3_62",
url="http://dx.doi.org/10.1007/978-3-319-16354-3_62"
}

@inproceedings{merck16cds,
  title={Semi-Supervised Information Retrieval System for Clinical Decision Support},
  author={Harsha Gurulingappa and Luca Toldo and Claudia Schepers and Alexander Bauer and Gerard Megaro},
  booktitle={TREC},
  year={2016}
}

@inproceedings{cbnu16cds,
  title={CBNU at TREC 2016 Clinical Decision Support Track},
  author={Seung-Hyeon Jo and Kyung-Soon Lee},
  booktitle={TREC},
  year={2016}
}

@inproceedings{eth16cds,
  author = "Greuter, Simon and Junker, Philip and Kuhn, Lorenz and Mance, Felix and Mermet, Virgile and Rellstab, Angela and Eickhoff, Carsten",
  editor = "Voorhees, Ellen M. and Ellis, Angela",
  title = "{E}{T}{H} {Z}urich at {T}{R}{E}{C} 2016 {C}linical {D}ecision {S}upport",
  booktitle = "TREC 2016",
  year = 2016,
}
  address = "S.l.",
  publisher = "National Institute of Standards and Technology",
  booktitle = "NIST Special Publication 500-321: The Twenty-Fifth Text REtrieval Conference Proceedings (TREC 2016)",

@article{kuhn16,
  title={Implicit Negative Feedback in Clinical Information Retrieval},
  author={Lorenz Kuhn and Carsten Eickhoff},
  journal={CoRR},
  year={2016},
  volume={abs/1607.03296}
}

@Article{koopman17,
author="Koopman, Bevan
and Russell, Jack
and Zuccon, Guido",
title="Task-oriented search for evidence-based medicine",
journal="International Journal on Digital Libraries",
year="2017",
pages="1--13",
abstract="Research on how clinicians search shows that they pose queries according to three common clinical tasks: searching for diagnoses, searching for treatments and searching for tests. We hypothesise, therefore, that structuring an information retrieval system around these three tasks would be beneficial when searching for evidence-based medicine (EBM) resources in medical digital libraries. Task-oriented (diagnosis, test and treatment) information was extracted from free-text medical articles using a natural language processing pipeline. This information was integrated into a retrieval and visualisation system for EBM search that allowed searchers to interact with the system via task-oriented filters. The effectiveness of the system was empirically evaluated using TREC CDS---a gold standard of medical articles and queries designed for EBM search. Task-oriented information was successfully extracted from 733,138 articles taken from a medical digital library. Task-oriented search led to improvements in the quality of search results and savings in searcher workload. An analysis of how different tasks affected retrieval showed that searching for treatments was the most challenging and that the task-oriented approach improved search for treatments. The most savings in terms of workload were observed when searching for treatments and tests. Overall, taking into account different clinical tasks can improve search according to these tasks. Each task displayed different results, making systems that are more adaptive to the clinical task type desirable. A future user study would help quantify the actual cost-saving estimates.",
issn="1432-1300",
doi="10.1007/s00799-017-0209-7",
url="http://dx.doi.org/10.1007/s00799-017-0209-7"
}

@inproceedings{goodwin16,
 author = {Goodwin, Travis R. and Harabagiu, Sanda M.},
 title = {Medical Question Answering for Clinical Decision Support},
 booktitle = {Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
 series = {CIKM '16},
 year = {2016},
 isbn = {978-1-4503-4073-1},
 location = {Indianapolis, Indiana, USA},
 pages = {297--306},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2983323.2983819},
 doi = {10.1145/2983323.2983819},
 acmid = {2983819},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {clinical decision support, medical information retrieval, question answering},
} 

@inproceedings{cds16overview,
  author    = {Kirk Roberts and
               Dina Demner{-}Fushman and
               Ellen M. Voorhees and
               William R. Hersh},
  title     = {Overview of the {TREC} 2016 Clinical Decision Support Track},
  booktitle = {Proceedings of The Twenty-Fifth Text REtrieval Conference, {TREC}
               2016, Gaithersburg, Maryland, USA, November 15-18, 2016},
  year      = {2016},
  url       = {http://trec.nist.gov/pubs/trec25/papers/Overview-CL.pdf},
  timestamp = {Wed, 08 Mar 2017 13:45:50 +0100},
  biburl    = {http://dblp2.uni-trier.de/rec/bib/conf/trec/RobertsDVH16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{clef16ehealthOverview,
author = {Kelly, Liadh and Goeuriot, Lorraine and Suominen, Hanna and Névéol, Aurélie and  Palotti, João and Zuccon, Guido},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-44564-9_24},
isbn = {9783319445632},
issn = {16113349},
keywords = {Entity linking,Evaluation,Information extraction,Information retrieval,Medical informatics,Nursing records,Patient handoff/handover,Self-diagnosis,Speech recognition,Test-set generation,Text classification,Text segmentation},
pages = {255--266},
title = {{Overview of the CLEF eHealth evaluation lab 2016}},
volume = {9822},
year = {2016}
}

@inproceedings{koopman11,
abstract = {This paper presents a framework for evaluating information retrieval of medical records. We use the BLULab corpus, a large collection of real-world de-identified medical records. The collection has been hand coded by clinical terminologists using the ICD-9 medical classification system. The ICD codes are used to devise queries and relevance judgements for this collection. Results of initial test runs using a baseline IR system show that there is room for improvement in medical information retrieval. Queries and relevance judgements are made available at http://aehrc.com/med{\_}eval},
author = {Koopman, Bevan and Bruza, Peter and Sitbon, Laurianne and Lawley, Michael},
booktitle = {Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval},
doi = {10.1145/2009916.2010088},
isbn = {978-1-4503-0757-4},
pages = {1139--1140},
title = {{Evaluating Medical Information Retrieval}},
year = {2011}
}

@article{edinger12,
abstract = {OBJECTIVE: Secondary use of electronic health record (EHR) data relies on the ability to retrieve accurate and complete information about desired patient populations. The Text Retrieval Conference (TREC) 2011 Medical Records Track was a challenge evaluation allowing comparison of systems and algorithms to retrieve patients eligible for clinical studies from a corpus of de-identified medical records, grouped by patient visit. Participants retrieved cohorts of patients relevant to 35 different clinical topics, and visits were judged for relevance to each topic. This study identified the most common barriers to identifying specific clinic populations in the test collection.$\backslash$n$\backslash$nMETHODS: Using the runs from track participants and judged visits, we analyzed the five non-relevant visits most often retrieved and the five relevant visits most often overlooked. Categories were developed iteratively to group the reasons for incorrect retrieval for each of the 35 topics.$\backslash$n$\backslash$nRESULTS: Reasons fell into nine categories for non-relevant visits and five categories for relevant visits. Non-relevant visits were most often retrieved because they contained a non-relevant reference to the topic terms. Relevant visits were most often infrequently retrieved because they used a synonym for a topic term.$\backslash$n$\backslash$nCONCLUSIONS: This failure analysis provides insight into areas for future improvement in EHR-based retrieval with techniques such as more widespread and complete use of standardized terminology in retrieval and data entry systems.},
author = {Edinger, Tracy and Cohen, Aaron M and Bedrick, Steven and Ambert, Kyle and Hersh, William},
doi = {10.1186/1471-2105-13-s14-s1},
isbn = {1942-597X (Electronic) 1559-4076 (Linking)},
issn = {1942-597X},
journal = {AMIA Annual Symposium Proceedings},
keywords = {Electronic Health Records,Humans,Information Storage and Retrieval,Patient Selection},
pages = {180--8},
pmid = {23304287},
title = {{Barriers to retrieving patient information from electronic health record data: failure analysis from the TREC Medical Records Track.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3540501{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {2012},
year = {2012}
}

@book{bruza08,
 author = {Bruza, Peter and Weeber, Marc},
 title = {Literature-based Discovery},
 year = {2008},
 isbn = {3540686851, 9783540686859},
 edition = {1},
 publisher = {Springer Publishing Company, Incorporated},
} 
